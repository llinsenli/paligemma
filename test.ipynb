{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 2-D convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 3, 224, 224])\n",
      "Output shape: torch.Size([4, 768, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the input tensor and convert it to float\n",
    "x = torch.arange(602112).reshape((4, 3, 224, 224)).float()\n",
    "\n",
    "# Define the convolutional layer\n",
    "cnn = nn.Conv2d(\n",
    "    in_channels = 3,\n",
    "    out_channels = 768,\n",
    "    kernel_size = 16,\n",
    "    stride = 16,\n",
    "    padding = 0  # 'valid' is not a valid value in PyTorch, use 0 for no padding\n",
    ")\n",
    "\n",
    "# Apply the convolutional layer to the input tensor\n",
    "output = cnn(x)\n",
    "\n",
    "# Output the shape of the resulting tensor\n",
    "'''\n",
    "input: (in_channels, image_size, image_size)\n",
    "output: (out_channels, (image_size-kernel_size)/stride + 1, image_size-kernel_size)/stride + 1)\n",
    "'''\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. tensor transpose, view, reshape, contiguous, expend, where, masked_scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 0],\n",
      "        [1, 0, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expend\n",
    "x = torch.tensor([[1, 1, 0], [1, 0, 0]])\n",
    "print(x)\n",
    "x.unsqueeze(-1).expand(-1,-1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.where\n",
    "# tensor.masked_scatter\n",
    "16*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare the input for the PaliGemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<image><image><image><image><image><image><image><image><image><image><bos>What is the person doing?\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_image_tokens_to_prompt(prefix_prompt, bos_token, image_seq_len, image_token):\n",
    "    # Quoting from the blog (https://huggingface.co/blog/paligemma#detailed-inference-process):\n",
    "    #   The input text is tokenized normally.\n",
    "    #   A <bos> token is added at the beginning, and an additional newline token (\\n) is appended.\n",
    "    #   This newline token is an essential part of the input prompt the model was trained with, so adding it explicitly ensures it's always there.\n",
    "    #   The tokenized text is also prefixed with a fixed number of <image> tokens.\n",
    "    # NOTE: from the paper it looks like the `\\n` should be tokenized separately, but in the HF implementation this is not done.\n",
    "    #       ref to HF implementation: https://github.com/huggingface/transformers/blob/7f79a97399bb52aad8460e1da2f36577d5dccfed/src/transformers/models/paligemma/processing_paligemma.py#L55-L73\n",
    "    return f\"{image_token * image_seq_len}{bos_token}{prefix_prompt}\\n\"\n",
    "\n",
    "\n",
    "prefix_prompt= \"What is the person doing?\"\n",
    "bos_token=\"<bos>\"\n",
    "image_seq_len=10\n",
    "image_token=\"<image>\"\n",
    "add_image_tokens_to_prompt(prefix_prompt, bos_token, image_seq_len, image_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[30523, 30523, 30523,   102,  2023,  2003,  1037,  2460,  6251,  1012,\n",
      "          1032,  1050,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [30523, 30523, 30523,   102,  2182,  2003,  1037,  3621,  2936,  6251,\n",
      "          2008,  2453,  2342, 19817,  4609, 10719,  2065,  2009, 23651,  1996,\n",
      "          4555,  3091,  1012],\n",
      "        [30523, 30523, 30523,   102,  2178,  2742,  6251,  1012,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [ 1032,  1050,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0]])\n",
      "Attention Masks: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# Initialize the DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "\n",
    "# Add special token\n",
    "special_tokens_dict = {\n",
    "    'additional_special_tokens': ['<image>', '<test>']\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# Extend the vocabulary with new words or symbols that should be treated as standard tokens\n",
    "EXTRA_TOKENS = ['<loc0000>', '<loc0001>', '<seg000>', '<seg001>']\n",
    "tokenizer.add_tokens(EXTRA_TOKENS)\n",
    "# Define a list of input strings\n",
    "input_strings = [\n",
    "    \"<image><image><image>[SEP]This is a short sentence.\\\\n\",\n",
    "    \"<image><image><image>[SEP]Here is a slightly longer sentence that might need truncation if it exceeds the maximum length.\",\n",
    "    \"<image><image><image>[SEP]Another example sentence.\\n\",\n",
    "    \"\\\\n\"\n",
    "]\n",
    "\n",
    "# Define parameters for padding and truncation\n",
    "padding = \"longest\"  # or \"max_length\"\n",
    "truncation = True  # or False\n",
    "# max_length = 10  # Set a max length for the sequences\n",
    "\n",
    "# Tokenize the input strings\n",
    "inputs = tokenizer(\n",
    "    input_strings,\n",
    "    return_tensors=\"pt\",  # Return as PyTorch tensors\n",
    "    padding=padding,      # Padding method\n",
    "    truncation=truncation,  # Truncation method\n",
    "    add_special_tokens=False # Ignore the \"[CLS]\", \"[SEP]\" token in default\n",
    ")\n",
    "\n",
    "# Print the tokenized outputs\n",
    "print(\"Input IDs:\", inputs['input_ids'])\n",
    "print(\"Attention Masks:\", inputs['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[[5., 5., 5., 5.],\n",
      "         [6., 6., 6., 6.]],\n",
      "\n",
      "        [[2., 4., 6., 8.],\n",
      "         [3., 5., 7., 9.]]])\n",
      "mean square: tensor([[[25.],\n",
      "         [36.]],\n",
      "\n",
      "        [[30.],\n",
      "         [41.]]])\n",
      "mean square root: tensor([[[0.2000],\n",
      "         [0.1667]],\n",
      "\n",
      "        [[0.1826],\n",
      "         [0.1562]]])\n",
      "normalized_output: tensor([[[1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[0.3651, 0.7303, 1.0954, 1.4606],\n",
      "         [0.4685, 0.7809, 1.0932, 1.4056]]])\n",
      "scaled_output: tensor([[[1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[0.3651, 0.7303, 1.0954, 1.4606],\n",
      "         [0.4685, 0.7809, 1.0932, 1.4056]]])\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-6\n",
    "weight = torch.tensor([0.0, 0.0, 0.0, 0.0]) #  nn.Parameter(torch.zeros(dim))\n",
    "x = torch.tensor([[[5.0, 5.0, 5.0, 5.0],\n",
    "                   [6.0, 6.0, 6.0, 6.0]],\n",
    "                  [[2.0, 4.0, 6.0, 8.0],\n",
    "                   [3.0, 5.0, 7.0, 9.0]]])\n",
    "\n",
    "print(f'x: {x}')\n",
    "means = x.pow(2).mean(-1, keepdim=True)\n",
    "print(f'mean square: {means}')\n",
    "rms = torch.rsqrt(means + eps)\n",
    "print(f'mean square root: {rms}')\n",
    "normalized_output = x * rms\n",
    "print(f'normalized_output: {normalized_output}')\n",
    "scaled_output = normalized_output * (1.0 + weight)\n",
    "print(f'scaled_output: {scaled_output}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. training mode in torch.nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x = nn.Linear(2, 3)\n",
    "print(x.training)\n",
    "x.eval()\n",
    "print(x.training)\n",
    "x.train()\n",
    "print(x.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, -1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paligemma from Scratch in PyTorch\n",
    "\n",
    "## Overview\n",
    "PaliGemma[^1] is a model that excels at interpreting and understanding both images and text. \n",
    "It's used for tasks like generating descriptions of images, answering questions based on visual content, \n",
    "and analyzing complex visuals like infographics and satellite images. The input to PaliGemma can be an \n",
    "image, text, or both, and the output might be a text description, an answer to a question, or information derived \n",
    "from the image. It's designed to handle a wide range of vision-language tasks efficiently, \n",
    "even though it is smaller in size compared to some other advanced models.\n",
    "\n",
    "The architecture of PaliGemma-3B, as shown in Figure~\\ref{fig:paligemma}, is inspired by the PaLI-3 model and combines the SigLIP visual encoder with the \n",
    "Gemma 2B language model. When PaliGemma-3B processes input, images are first converted into \"soft tokens\" by the SigLIP encoder. \n",
    "Simultaneously, any accompanying text, referred to as the \"prefix,\" is tokenized by Gemma's tokenizer. \n",
    "These image tokens and text tokens are then combined and fed into the Gemma decoder, which uses full block-attention to generate \n",
    "the final output text, or \"suffix,\" in an auto-regressive manner. \n",
    "\n",
    "\n",
    "## PaliGemma Architecture\n",
    "\n",
    "### SigLip (Vision Transformer)\n",
    "Describe the SigLip component, which utilizes a Vision Transformer architecture. Explain how it processes images or visual data and its role within the PaliGemma architecture.\n",
    "\n",
    "### Gemma (Language Model)\n",
    "Detail the Gemma component, focusing on its language model capabilities. Describe how it processes text data, what models it is based on, and its specific contributions to the overall project.\n",
    "\n",
    "### PaliGemma\n",
    "Discuss how the SigLip and Gemma components integrate to form the complete PaliGemma system. Highlight any unique interaction mechanisms or features that enhance the system’s performance.\n",
    "\n",
    "## Advanced Transformer Techniques\n",
    "\n",
    "### KV-Cache\n",
    "Explain the KV-Cache mechanism, its purpose, and how it improves the model’s efficiency or response time.\n",
    "\n",
    "### RMS Normalization\n",
    "Describe RMS Normalization, its theoretical basis, and its impact on model training and convergence.\n",
    "\n",
    "### Grouped Query Attention\n",
    "Detail the Grouped Query Attention technique, its algorithm, and its benefits for the model’s attention mechanism.\n",
    "\n",
    "### Rotary Positional Embedding\n",
    "Discuss the implementation and advantages of Rotary Positional Embedding within the transformer architecture.\n",
    "\n",
    "## How to Use\n",
    "Provide step-by-step instructions on how to set up, configure, and run the Paligemma project. Include any prerequisites, libraries, or environments that need to be installed or prepared.\n",
    "\n",
    "## Acknowledgements\n",
    "Offer thanks to those who contributed to the project, whether through direct development, advice, or inspiration.\n",
    "\n",
    "## References\n",
    "[^1]: Beyer, L., et al. (2024). *PaLI: A Jointly-Scaled Multilingual Language-Image Model*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
